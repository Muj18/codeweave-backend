{# templates/k8s_playbook.jinja2 #}
ðŸ“‹ **Template Scope:** This playbook provides comprehensive Kubernetes platform architecture for enterprise deployments. 
For quick fixes or simple tasks, use:
- **K8s Quick Task** â†’ immediate implementation guidance
- **K8s Quick Fix** â†’ emergency troubleshooting

This template delivers full cluster design, security hardening, compliance frameworks, and operational procedures suitable for CTO/platform team review.

---
{# STREAMING NOTE:
   If the model cannot finish due to length, it should end with EXACTLY: [CONTINUE_NEEDED] #}

You are a **staff-level Kubernetes Architect** producing a **production-grade Kubernetes Playbook**.  
This must reflect **deep Kubernetes expertise** suitable for CTO/CISO review and justify enterprise consulting rates.

Prompt: {{ prompt }}
Tool: Kubernetes

{% if context %}
Prior Conversation:
{{ context }}
{% endif %}

---

## 1) Executive Summary
- **Prompt:** {{ prompt | default("Design a production-grade Kubernetes platform.") }}  
- **Tool:** Kubernetes Playbook  
- **Cloud/Runtime:** {{ cloud | default("EKS + AKS + GKE (multi-cloud)") }}  
- **Prior Context:** {{ conversation | default("{}") }}  

This playbook defines **staff-level Kubernetes architecture** with enterprise-grade security, compliance, cost optimization, and operational excellence patterns.

---

## 2) Architecture Diagram
```mermaid
graph TD
    Dev[Developer] --> Repo[GitHub]
    Repo --> CI[GitHub Actions CI]
    CI --> ArgoCD[ArgoCD GitOps]
    ArgoCD --> Cluster[K8s Cluster (EKS/AKS/GKE)]
    Cluster --> CNI[CNI: Calico/Cilium/VPC-CNI]
    Cluster --> Ingress[Ingress: ALB/Nginx/Istio]
    Cluster --> CSI[Storage: EBS/AzureDisk/GCP-PD]
    Cluster --> RBAC[RBAC + PSS + OPA]
    Cluster --> Obs[Prometheus/Grafana/Loki]
    Cluster --> Cost[Kubecost + Autoscaler]
```

---

## 3) Production Cluster Configuration

### Complete EKS Setup (AWS)
```hcl
# EKS Cluster with Enterprise Security
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "20.8.4"

  cluster_name    = "prod-eks-${var.environment}"
  cluster_version = "1.29"

  vpc_id     = var.vpc_id
  subnet_ids = var.private_subnets

  # Security & Compliance
  enable_irsa = true
  kms_key_id  = aws_kms_key.eks.arn
  
  cluster_encryption_config = {
    provider_key_arn = aws_kms_key.eks.arn
    resources        = ["secrets"]
  }

  cluster_enabled_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]

  # Node Groups with Mixed Capacity
  eks_managed_node_groups = {
    spot_workers = {
      instance_types = ["m5.large", "m5.xlarge"]
      capacity_type  = "SPOT"
      min_size       = 2
      max_size       = 10
      desired_size   = 3
      
      k8s_labels = {
        workload = "general"
        capacity = "spot"
      }
      
      taints = {
        spot = {
          key    = "workload"
          value  = "spot"
          effect = "NO_SCHEDULE"
        }
      }
    }
    
    critical_workers = {
      instance_types = ["m5.xlarge"]
      capacity_type  = "ON_DEMAND"
      min_size      = 1
      max_size      = 3
      desired_size  = 2
      
      k8s_labels = {
        workload = "critical"
        capacity = "on-demand"
      }
    }
  }

  # Cluster Add-ons
  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
      configuration_values = jsonencode({
        env = {
          ENABLE_POD_ENI = "true"
          ENABLE_PREFIX_DELEGATION = "true"
        }
      })
    }
    aws-ebs-csi-driver = {
      most_recent = true
      service_account_role_arn = aws_iam_role.ebs_csi.arn
    }
  }
}

# KMS Key for EKS
resource "aws_kms_key" "eks" {
  description             = "EKS Cluster Encryption Key"
  deletion_window_in_days = 7
  enable_key_rotation     = true
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid    = "Enable IAM User Permissions"
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"
        }
        Action   = "kms:*"
        Resource = "*"
      }
    ]
  })
}

# OIDC Provider for Workload Identity
resource "aws_iam_openid_connect_provider" "eks" {
  url = module.eks.cluster_oidc_issuer_url
  client_id_list = ["sts.amazonaws.com"]
  thumbprint_list = [data.tls_certificate.eks_oidc.certificates[0].sha1_fingerprint]
}
```

### Production Workload with Security
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-service
  namespace: production
  labels:
    app: api-service
    tier: backend
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: api-service
  template:
    metadata:
      labels:
        app: api-service
        tier: backend
    spec:
      serviceAccountName: api-service-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: api
        image: myregistry/api-service:v1.2.3
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        resources:
          requests:
            cpu: "250m"
            memory: "512Mi"
            ephemeral-storage: "1Gi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
            ephemeral-storage: "2Gi"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir:
          sizeLimit: "1Gi"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: [api-service]
              topologyKey: kubernetes.io/hostname
      tolerations:
      - key: workload
        operator: Equal
        value: spot
        effect: NoSchedule
      nodeSelector:
        workload: general

---
apiVersion: v1
kind: Service
metadata:
  name: api-service
  namespace: production
spec:
  selector:
    app: api-service
  ports:
  - port: 80
    targetPort: 8080
    name: http
  type: ClusterIP

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-service-pdb
  namespace: production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api-service

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-service-netpol
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53
```

### Storage Classes for Different Workloads
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
  kmsKeyId: "arn:aws:kms:us-west-2:123456789012:key/eks-storage"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cheap-storage
provisioner: ebs.csi.aws.com
parameters:
  type: st1
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
```

---

## 4) RBAC & Security Hardening

### Pod Security Standards
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
    
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-service-sa
  namespace: production
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/api-service-role"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: api-service-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: api-service-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: api-service-sa
  namespace: production
roleRef:
  kind: Role
  name: api-service-role
  apiGroup: rbac.authorization.k8s.io
```

### OPA Gatekeeper Policy
```yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredsecuritycontext
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredSecurityContext
      validation:
        openAPIV3Schema:
          type: object
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredsecuritycontext
        violation[{"msg": msg}] {
          input.review.object.kind == "Pod"
          not input.review.object.spec.securityContext.runAsNonRoot
          msg := "Pod must run as non-root user"
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredSecurityContext
metadata:
  name: must-run-as-non-root
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    excludedNamespaces: ["kube-system", "kube-public"]
```

---

## 5) Observability & Cost Management

### Prometheus Configuration
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "/etc/prometheus/rules/*.yml"
    
    scrape_configs:
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        target_label: __address__
        replacement: '${1}:9100'
    
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  kubernetes.yml: |
    groups:
    - name: kubernetes
      rules:
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[5m]) * 60 * 5 > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
```

---

## 6) Operational Runbook

### Cluster Lifecycle Management
```bash
#!/bin/bash
# scripts/cluster_ops.sh

cluster_upgrade() {
    local cluster_name=$1
    local target_version=$2
    
    echo "Upgrading cluster ${cluster_name} to ${target_version}"
    
    # Pre-upgrade validation
    kubectl get nodes -o wide
    kubectl get pods --all-namespaces | grep -v Running
    
    # Drain and upgrade nodes in batches
    for node in $(kubectl get nodes -o name); do
        echo "Draining ${node}"
        kubectl drain ${node} --ignore-daemonsets --delete-emptydir-data --force
        
        # Wait for pods to reschedule
        sleep 60
        
        # Upgrade node (cloud-specific)
        aws eks update-nodegroup-version --cluster-name ${cluster_name} --nodegroup-name general
        
        # Wait for node to be ready
        kubectl wait --for=condition=Ready ${node} --timeout=300s
        kubectl uncordon ${node}
    done
}

backup_etcd() {
    local backup_name="etcd-backup-$(date +%Y%m%d-%H%M%S)"
    
    # Create etcd snapshot
    kubectl -n kube-system exec etcd-master -- \
        etcdctl --endpoints=https://127.0.0.1:2379 \
        --cacert=/etc/kubernetes/pki/etcd/ca.crt \
        --cert=/etc/kubernetes/pki/etcd/server.crt \
        --key=/etc/kubernetes/pki/etcd/server.key \
        snapshot save /var/lib/etcd/${backup_name}.db
        
    # Upload to S3
    kubectl cp kube-system/etcd-master:/var/lib/etcd/${backup_name}.db ./
    aws s3 cp ${backup_name}.db s3://k8s-backups/etcd/
}
```

---

## 7) Cost Optimization Framework

### Kubecost Integration
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubecost-cost-model
  namespace: kubecost
data:
  config.yaml: |
    spotLabel: "capacity"
    spotLabelValue: "spot"
    gpuLabel: "accelerator"
    
    # Cloud provider configs
    cloudProvider: "AWS"
    awsRegion: "us-west-2"
    
    # Custom pricing for accurate cost allocation
    customPricing:
      spotCPU: 0.01
      spotRAM: 0.005
      onDemandCPU: 0.04
      onDemandRAM: 0.02

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cost-optimizer
  namespace: kubecost
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cost-optimizer
  template:
    metadata:
      labels:
        app: cost-optimizer
    spec:
      containers:
      - name: optimizer
        image: kubecost/cost-optimizer:latest
        env:
        - name: CLUSTER_NAME
          value: "prod-eks"
        - name: RECOMMENDATIONS_ENABLED
          value: "true"
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
```

---

## 8) Security & Compliance Implementation

### Network Policies (Zero Trust)
```yaml
# Default deny-all policy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# Allow specific communication patterns
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-to-database
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api-service
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
  # Allow DNS
  - to: []
    ports:
    - protocol: UDP
      port: 53
```

---

## 9) Risks & Enterprise Mitigations

| Risk | Kubernetes-Specific Impact | Enterprise Mitigation |
|------|----------------------------|----------------------|
| etcd corruption | Cluster state loss, total outage | Automated snapshots + cross-region backup |
| Node pool failure | Workload eviction, capacity loss | Multi-AZ node groups + cluster autoscaler |
| RBAC misconfiguration | Security gaps or blocked deployments | OPA policies + regular audit + break-glass procedures |
| Network policy drift | Compliance violations, lateral movement | GitOps enforcement + Cilium/Calico monitoring |
| Image vulnerabilities | Security compliance failure | Admission controllers + Trivy scanning + policy enforcement |
| Resource quota exhaustion | Pod scheduling failures | Monitoring + auto-scaling + cost allocation by team |

---

## 10) Quick Wins & KPIs

### Immediate Implementation (Week 1)
- Enable **Cluster Autoscaler** with 70% spot nodes â†’ 30-40% cost reduction
- Apply **Pod Security Standards** restricted mode â†’ compliance baseline
- Deploy **Kubecost** â†’ namespace cost visibility and chargeback
- Implement **NetworkPolicies** in critical namespaces â†’ zero trust
- Add **resource quotas** per namespace â†’ prevent resource hogging

### Success Metrics
- **Availability SLO:** 99.9% uptime (measured per service)
- **Cost Efficiency:** <$0.50 per pod per day (including overhead)
- **Security Posture:** 100% pods compliant with Pod Security Standards
- **Performance:** p95 pod startup time <30 seconds
- **Compliance:** Zero audit findings for PCI/HIPAA controls

---

## 11) Implementation Checklist

### Phase 1: Foundation (Week 1-2)
- [ ] Deploy cluster with Terraform configuration above
- [ ] Configure RBAC and Pod Security Standards
- [ ] Set up basic monitoring (Prometheus/Grafana)
- [ ] Implement network policies

### Phase 2: Production Readiness (Week 3-4)
- [ ] Configure backup and disaster recovery procedures
- [ ] Deploy cost monitoring and optimization tools
- [ ] Set up comprehensive observability stack
- [ ] Implement compliance scanning and reporting

### Phase 3: Operational Excellence (Week 5-6)
- [ ] Establish cluster upgrade procedures
- [ ] Create chaos engineering tests
- [ ] Document operational runbooks
- [ ] Train team on troubleshooting procedures

This Kubernetes platform delivers enterprise-grade container orchestration with security, compliance, and cost optimization built-in from day one.

[END OF TEMPLATE]