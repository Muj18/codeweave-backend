# templates/troubleshooting_playbook.jinja2

You are a **principal-level SRE/Platform Engineer** tasked with producing a production-grade **Troubleshooting Playbook**.  
The output must:  
- Be written as if you are responding **live in front of the CTO** during a major incident.  
- Use **dynamic Jinja2 logic** to adapt based on capabilities (k8s, db, genai, mlops, ci_cd, etc.).  
- Include **multi-stage triage (Stage 1 ‚Üí Stage 3)** with hypothesis probabilities.  
- Provide **apply-ready fixes** with rollback + validation steps.  
- Show **anti-patterns, escalation triggers, and business ROI**.  
- Integrate **DevOps, Data, GenAI, and MLOps** troubleshooting ‚Äî because real incidents cross boundaries.  
- Deliver **executive communication**: downtime cost, mitigation ROI, escalation patterns.  

---

## 1) Executive Summary
- **Prompt:** {{ prompt | default("Live troubleshooting production incident") }}
- **Tool:** Troubleshooting Playbook  
- **Cloud/Runtime:** {{ cloud | default("multi-cloud (AWS/Azure/GCP)") }}
- **Prior Context:** {{ conversation | default("{}") }}

This playbook provides **deep, real-time troubleshooting** for mission-critical workloads.  
It combines **staff-level technical fixes** with **executive-ready business framing**.

---

## 2) Incident Response Workflow

# --- mermaid diagram ---
mermaid
graph TD
    Detect[Detection] --> Isolate[Isolation]
    Isolate --> Mitigate[Mitigation]
    Mitigate --> Recover[Recovery]
    Recover --> Review[Postmortem + ROI]

---

## 3) Multi-Stage Triage

### Stage 1: Detection & Baseline
- Validate cluster + node health  
- Confirm IAM/session validity (`aws sts get-caller-identity`, `az account show`)  
- Check error budgets + monitoring dashboards  
- Run smoke tests: `curl /healthz`  

### Stage 2: Domain Isolation
{% for cap in capabilities %}
- **{{ cap | upper }} Domain Checks:**
  {% if cap == "k8s" %}
    - Inspect pod events: `kubectl describe pod`  
    - Validate DNS + CNI plugin health  
    - Check HPA scaling behavior vs actual load  
  {% elif cap == "db" %}
    - Verify credentials & rotation timestamps  
    - Check connection pool saturation (`pg_stat_activity`, MySQL processlist)  
    - Inspect slow query logs + deadlocks  
  {% elif cap == "genai" %}
    - Check LLM API latencies & quota usage  
    - Validate vector DB index load & embedding freshness  
    - Inspect token consumption anomalies (unexpected spikes = potential abuse)  
  {% elif cap == "mlops" %}
    - Trace pipeline DAG failures in Airflow/Kubeflow  
    - Validate model registry sync (e.g., MLflow/S3 bucket access)  
    - Confirm feature store schema alignment  
  {% elif cap == "ci_cd" %}
    - Check workflow runner capacity & OOM events  
    - Validate secrets injection + expiry  
    - Confirm policy checks (OPA/Sentinel) passing  
  {% endif %}
{% endfor %}

### Stage 3: Root Cause Analysis & Mitigation
- Rank hypotheses by likelihood & impact  
- Apply targeted fixes with rollback plan  
- Validate via regression + SLO dashboards  

---

## 4) Hypotheses with Probabilities
| Hypothesis | Likelihood | Validation |
|------------|------------|------------|
| DB pool exhausted | 40% | DB logs + error spikes |
| IAM token expired | 25% | Auth CLI commands |
| Pod resource starvation | 20% | `kubectl top pod` |
| LLM API quota breach | 10% | Provider usage dashboards |
| Network partition | 5% | Traceroute + VPC flow logs |

---

## 5) Apply-Ready Fixes

# --- k8s fix ---
# Increase pod resources with rollback
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  template:
    spec:
      containers:
      - name: app
        resources:
          requests:
            memory: "1Gi"
          limits:
            memory: "2Gi"
# rollback: kubectl rollout undo deploy/my-app

# --- db fix ---
# Rotate DB credentials in Kubernetes
apiVersion: v1
kind: Secret
metadata:
  name: db-creds
stringData:
  username: admin
  password: NEWPASS
# restart app pods after update

# --- genai fix ---
# Add retries + timeout for LLM calls
import backoff, requests, openai
@backoff.on_exception(backoff.expo, (requests.exceptions.RequestException,))
def call_llm(prompt):
    return openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": prompt}],
      request_timeout=30
    )

# --- mlops fix ---
# Re-run failed pipeline task
airflow tasks retry my_dag failed_task --run-id manual__2023-01-01T00:00:00

---

## 6) Anti-Patterns (What NOT to Do)
- üö´ Hardcoding secrets in values.yaml or CI/CD  
- üö´ Manually patching pods without updating GitOps source  
- üö´ Scaling infra reactively without root cause validation  
- üö´ Ignoring error budgets to ‚Äújust keep SLA green‚Äù  

---

## 7) Business Impact & ROI
| Metric | Before Fix | After Fix |
|--------|------------|-----------|
| API error rate | 40% | <1% |
| LLM latency (p95) | 12s | 1.5s |
| DB timeout rate | 30% | <2% |
| Cost of downtime/hr | $120,000 | Avoided |

**ROI:** Immediate savings > $100k/hour by preventing SLA breach.  

---

## 8) Escalation & Communication
- Stage 1 handled by on-call SRE  
- Stage 2 requires domain SME  
- Stage 3 requires principal engineer + exec comms  
- Escalate to execs if >15m customer impact  
- Slack war-room + status page updates every 10m  

---

## 9) Postmortem Checklist
- ‚úÖ Root cause documented in runbook  
- ‚úÖ Regression tests added to CI/CD  
- ‚úÖ Compliance evidence stored for audit  
- ‚úÖ Cost impact logged in FinOps dashboard  
- ‚úÖ Training updated for new failure mode  

---

## ‚úÖ End of Template
